# Coldplay Music Generation AI

This project uses a GPT-2 model, fine-tuned on a dataset of Coldplay MIDI files, to generate new, original piano music in the style of Coldplay. The entire pipeline, from data preprocessing to model training and music generation, is included.

The project is built using PyTorch, Hugging Face Transformers, and the `miditok` library for MIDI tokenization.

## Features
- **MIDI Preprocessing**: Converts a collection of MIDI files into a tokenized format suitable for training.
- **GPT-2 Training**: Fine-tunes a custom GPT-2 model from scratch on the processed music data.
- **Music Generation**: Uses the trained model to generate new musical sequences and saves them as MIDI files.
- **Audio Synthesis**: Optionally converts the generated MIDI file into a `.wav` audio file using a SoundFont.

## Project Structure
```
Cold_AI/
├── data/
│   ├── midi/               # Place your input MIDI files (.mid) here
│   └── all_tokens.json     # Generated by the preprocessing script
├── models/
│   ├── coldplay_gpt2/      # Trained model is saved here
│   └── tokenizer.json
├── outputs/                # Generated MIDI and WAV files appear here
├── src/
│   ├── preprocess.py       # Script to tokenize MIDI files
│   ├── train.py            # Script to train the GPT-2 model
│   └── generate.py         # Script to generate new music
├── .gitignore              # Specifies files for Git to ignore
├── requirements.txt        # Python dependencies
└── run.sh                  # Master script to run the full pipeline
```

## Setup and Installation

**1. Clone the repository:**
```bash
https://github.com/DipayanDasgupta/ColdplAI.git
cd ColdplAI
```

**2. Create and activate a Python virtual environment:**
```bash
python3 -m venv venv
source venv/bin/activate
```

**3. Install dependencies:**
```bash
pip install -r requirements.txt
```

**4. (Optional) Install FluidSynth for audio synthesis:**
To convert the final `.mid` file to a `.wav` file, you need FluidSynth and a SoundFont.
```bash
# On Debian/Ubuntu
sudo apt-get update
sudo apt-get install fluidsynth
```
You will also need to download a SoundFont file (e.g., [GeneralUser GS](https://schristiancollins.com/generaluser.php)) and update the `SOUNDFONT_PATH` in `src/generate.py`.

## How to Use

**1. Prepare Your Data:**
Place all your training MIDI files (with `.mid` or `.midi` extension) inside the `data/midi/` directory.

**2. Run the Full Pipeline:**
The `run.sh` script automates all three steps: preprocessing, training, and generation.

First, make the script executable:
```bash
chmod +x run.sh
```
Then, run it:
```bash
./run.sh
```
The final generated song will be in the `outputs/` directory.

**3. Running Steps Individually:**
You can also run each step of the process separately.

- **Preprocess Data:** `python src/preprocess.py`
- **Train Model:** `python src/train.py`
- **Generate Music (after training):** `python src/generate.py`

## Configuration
You can customize the music generation process by editing the variables at the top of `src/generate.py`:
- `MAX_NEW_TOKENS`: The length of the generated song (in tokens).
- `TEMPERATURE`: Controls randomness. Higher values (~1.0) mean more creative/chaotic, lower values (~0.7) mean more predictable.
- `TOP_K`: Narrows the model's choices to the `k` most likely next tokens.

## License
This project is licensed under the MIT License.